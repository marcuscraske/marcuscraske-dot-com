General themes:
- rust on the bandwagon
- idoponant queues
- microservice are representative of shift in organisational structure
- software eating the world
- you will ship your org (structure)
- microservices allow shipping faster with much less blast radius and risk
- logging everything is not realistic, should be sampling and holding counters to detect issues/produce metrics, and turn on finer logging when issues pop-up. Disagreement made in Kafka talk (day 3) for critical systems, when need full picture.

Day 1
------------
Keynote on microservices:
- FT ripped absolute shit out of effectively the old way Worldpay works, with: SOEs,. change management process
- new process went from release every month to thousands of much smaller autonomous releases every month
- less impact/ incidents from releases

Air BnB migration at scale:
- Data services only point to access io, i.e. only single route to a database or cache
- Presentation layer for rendering separately
- Staging replays production traffic
Diffy stage before canary - GitHub Twitter diffy - compares output of production and new version of service before rollout
-  decision made at scale by having infrastructure working group consisting of senior engineers
- started migration from bottom up, with data services first (good pyramid on slides)
- templating important for adoption and scale between teams
- local development works by running dev version of services, teams just hit them (APIs) like blackbox, can run them locally - almost like simulators for ftes. Teams responsible for service are responsible for running them.

Cloudflare workers:
- securityheaders.io
- use cases: jwt validation, looking for secrets
- discord using entirely cloudflare workers
- a/b routing and balancing manipulation layer

Future of Java:
- list.of - pretty useful, similar available for maps
- jshell - run code like bash in terminal
- jlink - optimize container image sizes
- http client in Java 11 (sexy)
- switch expressions Java 12 can be written differently, no need for breaks

Lessons from Google:
- think of microservices as organisational change
- hippy's vs ants with regards to balancing autonomy of process/tooling etc
- services can be too small, network isn't free

Paybase (on microservices):
- stable observability stack important
- use chaos engineering
- engineers closer to pci-dss process, seem to get around lot of paper work we get mandated from above
- microscanner used for image scanning

Day 2
---------------
Progressive delivery:
- worth selling to business as next step after continuous delivery (where we kinda are now)
- deploy != Release, point is controlled roll out and observability
- pagerduty, ops genie
- selling to business: itil / change management culture has lack of empathy for both customers and team.

Web assembly:
- effectively equivalent of byte code layer for web browsers, reduces need to compile and optimise JavaScript (v8 engine)
- webassembly.studio
- toolchains: emscripten (llvm), rustwasm, assemblyscript (typescript) (order of tools from low/complex to entry level)
- needs bindings to talk with js? Uwotm8

Microservices (Tesla):
- Colin Breck good blog apparently
- store snapshots of metrics/kpis from micro services into time series databafses
- real war case: persistent volume couldn't be detached due to worker not exiting, thus nothing written to logs. Thus need to look at absence.
- re-enforced that services should own their data and not share data (obvious but important) - no replication of validation and error case scenarios etc
- services need to handle failure in business logic: exponential backoff. Automatic restarting of services after idle time.
- Ingres behaviour important: auto-retry could knock out entire service, one pod after another after retrying same request.
- blog has essay on k8 on readiness probes

300k lines of infrastructure code:
- yak shaving (term)
- automated infrastructure testing: bring it up, check it works, shut it down
- equivalent to develop jobs today
- run automation, bang it all together with Cron and triggered (on commit) job

Airbnb scaling:
- using per env configmaps
- deployment config lives with code, but using framework to generate generic config - mixed thoughts, seems like band-aid for infrastructure code not scaling well
- framework constantly being tested by ci automatically (creating new services etc all the time)
- set of python scripts to automate refactoring, such as upgrading base image used across the fleet by hundreds of services
- kubectl plugins - generally seems like industry using straight up k8, openshift gonna make extensibility interesting... :)
- Google container tools: krew - plugin for installing kubectl plugins
- k command? Kubectl? Using it to build their deployment images. Builds are also ran inside a container, bit like openshift tbf.
- recommended talk on optimising Java containers from earlier today
- forced validation checks during build and deployment, almost like idea of gates
- standardised manage namespace name to include env e.g. app-production
- gitops workflow

Observability:
- not same as monitoring
- monitoring, alerting, tracing in part
- mainly engineering culture based on facts
- victorops, honeycomb, datadog, fluentd, humio
- context should include information about where service is running i.e. container az zone, name etc. TBF we're pretty good at this already, may just need slightly more metadata in container environment.
- example of real log was structured json with lots of information about the request.


Day 3
-----------
Keynote microservices:
- you will ship your org
- risk of not knowing service ownership and poor communication between teams, and leading to unexpected scaling issues due to unplanned / unforseen volume of traffic
- almost impossible to log and archive all data at huge scale (lol splunk), realistically need to sample
- need to separate tooling used for immediate alerting and long term log analysis
- keep sli's / golden metrics simple: req rate, error rate, latency
- need graph of pXX latency, single glass pane
- p95 / p99
- time series histogram of p95 over time, or bins of latency in the present
- worth a look at lightstep/dapper product
- critical path analysis to help optimise latency of entire request through microservices, via e.g. looking at outliers, help find potential bottlenecks
- distributed tracing tooling important

React state alternative:
- "simmply" - need to go over repo again
- Twitter: r31gN_
- react context api

Reactive networking:
- rsocket
- bidir messaging between services, binary format (efficient), message level flow control, abstract from transport layer, streams reduce sockets, initiated by client.
- facebook: their logging samples, but able to turn on full logging / telemetry for individual users
- Alibaba: similar. Basically holding connection open, which virtually hits api multiple times with sockets, but instead uses channels.
- main point: APIs change without changing streams or connections - p cool
- spring support coming up very soon, with message handling

Real world Kafka (streaming analytics):
- humio - log search/store tool, impressive, runs onprem only.
- their opinion for mission critical is log everything, rather than sample
- similar to splunk, showed real production system in real time (fo real) under outside (brute) attack
- effectively using Kafka like jms queue
- sub-second replication/processing/digest due to Kafka, for few Tb of data per day
